{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import torch # type: ignore\n",
    "import torch.nn as nn # type: ignore \n",
    "import pandas as pd # type: ignore\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    aggregation = data.new_full((num_segments, data.size(1)), 0)\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    aggregation.scatter_add_(dim=0, index=segment_ids, src=data)\n",
    "    return aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGCL(nn.Module):\n",
    "    '''\n",
    "    Equivariant Graph Convolutional Layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_node_features, hidden_node_features, output_node_features, activation_function=nn.SiLU(), include_residual=True, use_normalization=True):\n",
    "        super(EGCL, self).__init__()\n",
    "        self.radial_num_features = 1\n",
    "        self.input_node_features = input_node_features\n",
    "        self.hidden_node_features = hidden_node_features\n",
    "        self.output_node_features = output_node_features\n",
    "        self.include_residual = include_residual\n",
    "        self.use_normalization = use_normalization\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def coord_to_radial(self, r, edge_index):\n",
    "        ''' \n",
    "        INPUT: r (M x 3): coordinate matrix containing 3D coordinates for each of the M atoms in the point cloud\n",
    "               edge_index (2 x num_edges): 2 lists containing corresponding source and target atoms\n",
    "        \n",
    "        OUTPUT: radials (num_edges x 1): tensor containing distance radials for each pair of connected atoms\n",
    "                coordinate_differences (num_edges x 3): tensor containing coordinate differences for each pair of connected atoms\n",
    "        '''\n",
    "        source_node_indices, target_node_indices = edge_index[0], edge_index[1]\n",
    "        coordinate_differences = r[source_node_indices] - r[target_node_indices]\n",
    "        # sum the squared differences of the each pair of corresponding distance components\n",
    "        radials = torch.sum(coordinate_differences ** 2, dim=1).unsqueeze(1) # use unsqueeze to keep the dimension that torch.sum gets rid of\n",
    "        return coordinate_differences, radials, torch.sqrt(radials)\n",
    "\n",
    "\n",
    "    def forward(self, r, h, u_mask, edge_index):\n",
    "        '''\n",
    "        r (M x 3): coordinate matrix containing coordinates for each of the M atoms of the point cloud\n",
    "        h (M x node_features): feature matrix containing feature embeddings for each fo the M atoms of the point cloud\n",
    "        '''\n",
    "\n",
    "        coordinate_differences, radials, radials_sqrt = self.coord_to_radial(r, edge_index)\n",
    "\n",
    "\n",
    "        #----------EDGE MODEL----------\n",
    "        e_mlp = None\n",
    "        if self.use_normalization == False:\n",
    "            e_mlp = nn.Sequential(\n",
    "                nn.Linear(self.input_node_features * 2 + self.radial_num_features, self.hidden_node_features),\n",
    "                self.activation_function,\n",
    "                nn.Linear(self.hidden_node_features, self.hidden_node_features),\n",
    "            )\n",
    "        else:\n",
    "            e_mlp = nn.Sequential(\n",
    "                nn.BatchNorm1d(self.input_node_features * 2 + self.radial_num_features),\n",
    "                nn.Linear(self.input_node_features * 2 + self.radial_num_features, self.hidden_node_features),\n",
    "                self.activation_function,\n",
    "                nn.BatchNorm1d(self.hidden_node_features),\n",
    "                nn.Linear(self.hidden_node_features, self.hidden_node_features),\n",
    "            )\n",
    "        \n",
    "\n",
    "        source_node_indices, target_node_indices = edge_index[0], edge_index[1]\n",
    "        source_node_h_embeddings = h[source_node_indices]\n",
    "        target_node_h_embeddings = h[target_node_indices]\n",
    "\n",
    "        e_mlp_input = torch.cat([source_node_h_embeddings, target_node_h_embeddings, radials], dim=1)\n",
    "        m = e_mlp(e_mlp_input)\n",
    "\n",
    "\n",
    "        #----------NODE FEATURE MODEL----------\n",
    "        h_mlp = None\n",
    "        if self.use_normalization == False:\n",
    "            h_mlp = nn.Sequential(\n",
    "                nn.Linear(self.input_node_features + self.hidden_node_features),\n",
    "                self.activation_function,\n",
    "                nn.Linear(self.hidden_node_features, self.output_node_features)\n",
    "            )\n",
    "        else:\n",
    "            h_mlp = nn.Sequential(\n",
    "                nn.BatchNorm1d(self.input_node_features + self.hidden_node_features),\n",
    "                nn.Linear(self.input_node_features + self.hidden_node_features, self.hidden_node_features),\n",
    "                self.activation_function,\n",
    "                nn.BatchNorm1d(self.hidden_node_features),\n",
    "                nn.Linear(self.hidden_node_features, self.output_node_features)\n",
    "            )\n",
    "\n",
    "        # implement aggregation to perform message passing between nodes\n",
    "        agg = unsorted_segment_sum(data=m, segment_ids=source_node_indices, num_segments=h.size(0))\n",
    "        h_mlp_input = torch.cat([h, agg])\n",
    "        h_updated = h_mlp(h_mlp_input)\n",
    "        \n",
    "        if self.include_residual == True:\n",
    "            h_updated = h + h_updated\n",
    "        h = h_updated\n",
    "\n",
    "        #----------COORDINATE MODEL----------\n",
    "        r_mlp = None\n",
    "        if self.use_normalization == False:\n",
    "            r_mlp = nn.Sequential(\n",
    "                nn.Linear(self.input_node_features * 2 + self.radial_num_features, self.hidden_node_features),\n",
    "                self.activation_function,\n",
    "                nn.Linear(self.hidden_node_features, 1)\n",
    "            )\n",
    "        else:\n",
    "            r_mlp = nn.Sequential(\n",
    "                nn.BatchNorm1d(self.input_node_features * 2 + self.radial_num_features),\n",
    "                nn.Linear(self.input_node_features * 2 + self.radial_num_features, self.hidden_node_features),\n",
    "                self.activation_function,\n",
    "                nn.BatchNorm1d(self.hidden_node_features),\n",
    "                nn.Linear(self.hidden_node_features, 1)\n",
    "            )\n",
    "        \n",
    "        r_mlp_input = e_mlp_input\n",
    "        r_mlp_output = r_mlp(r_mlp_input)\n",
    "\n",
    "        vel_agg_components = (coordinate_differences / (radials_sqrt + 1)) * r_mlp_output\n",
    "        vel_agg = unsorted_segment_sum(data=vel_agg_components, segment_ids=source_node_indices, num_segments=r.size(0))\n",
    "        r = r + vel_agg * u_mask\n",
    "\n",
    "\n",
    "        return r, h\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNN(nn.Module):\n",
    "    def __init__(self, input_node_features, hidden_node_features, output_node_features, num_layers=4, use_residual=True, use_normalization=True):\n",
    "        super(EGNN, self).__init__()\n",
    "\n",
    "        self.input_embedding = nn.Linear(input_node_features, hidden_node_features)\n",
    "        self.output_embedding = nn.Linear(hidden_node_features, output_node_features)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        for l in range(num_layers):\n",
    "            self.add_module(f\"EGCL{l}\", EGCL(\n",
    "                input_node_features=hidden_node_features, \n",
    "                hidden_node_features=hidden_node_features, \n",
    "                output_node_features=hidden_node_features, \n",
    "                include_residual=use_residual, \n",
    "                use_normalization=use_normalization\n",
    "            ))\n",
    "\n",
    "    def forward(self, r, h, u_mask, edge_index):\n",
    "        h = self.input_embedding(h)\n",
    "        for l in range(self.num_layers):\n",
    "            r, h = self._modules[f\"EGCL{l}\"](r, h, u_mask, edge_index)\n",
    "        h = self.output_embedding(h)\n",
    "        \n",
    "        return r, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkerSizeGCL(nn.Module):\n",
    "    def __init__(self, input_node_features, hidden_node_features, output_node_features, use_residual=True, use_normalization=True, activation_function=nn.ReLU()):\n",
    "        super(LinkerSizeGCL, self).__init__()\n",
    "        self.num_edge_attribute_features = 1\n",
    "        self.use_residual = use_residual\n",
    "        \n",
    "        self.e_mlp = None\n",
    "        if use_normalization == False:\n",
    "            self.e_mlp = nn.Sequential(\n",
    "                nn.Linear(2 * input_node_features + self.num_edge_attribute_features, hidden_node_features),\n",
    "                activation_function,\n",
    "                nn.Linear(hidden_node_features, hidden_node_features)\n",
    "            )\n",
    "        else:\n",
    "            self.e_mlp = nn.Sequential(\n",
    "                nn.BatchNorm1d(2 * input_node_features + self.num_edge_attribute_features),\n",
    "                nn.Linear(2 * input_node_features + self.num_edge_attribute_features, hidden_node_features),\n",
    "                activation_function,\n",
    "                nn.BatchNorm1d(hidden_node_features),\n",
    "                nn.Linear(hidden_node_features, hidden_node_features)\n",
    "            )\n",
    "\n",
    "        self.h_mlp = None\n",
    "        if use_normalization == False:\n",
    "            self.h_mlp = nn.Sequential(\n",
    "                nn.Linear(input_node_features + hidden_node_features, hidden_node_features),\n",
    "                activation_function,\n",
    "                nn.Linear(hidden_node_features, output_node_features)\n",
    "            )\n",
    "        else:\n",
    "            self.h_mlp = nn.Sequential(\n",
    "                nn.BatchNorm1d(input_node_features + hidden_node_features),\n",
    "                nn.Linear(input_node_features + hidden_node_features, hidden_node_features),\n",
    "                activation_function,\n",
    "                nn.BatchNorm1d(hidden_node_features),\n",
    "                nn.Linear(hidden_node_features, output_node_features)\n",
    "            )\n",
    "\n",
    "    def forward(self, a, h, edge_index):\n",
    "        # a is the edge feature matrix. it only has one column for each edge containing the distance for that edge\n",
    "        source_node_indices, target_node_indices = edge_index[0], edge_index[1]\n",
    "        source_node_embeddings, target_node_embeddings = h[source_node_indices], h[target_node_indices]\n",
    "        \n",
    "        e_mlp_input = torch.cat([source_node_embeddings, target_node_embeddings, a], dim=1)\n",
    "        m = self.h_mlp(e_mlp_input)\n",
    "\n",
    "        agg = unsorted_segment_sum(data=m, segment_ids=source_node_indices, num_segments=h.size(0))\n",
    "        h_mlp_input = torch.cat([h, agg], dim=1)\n",
    "        h_updated = self.h_mlp(h_mlp_input)\n",
    "        if self.use_residual == True:\n",
    "            h_updated = h + h_updated\n",
    "        h = h_updated\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkerSizeGNN(nn.Module):\n",
    "    def __init__(self, input_node_features, hidden_node_features, output_node_features, use_normalization=True, use_residual=True, num_layers=4, activation_function=nn.ReLU()):\n",
    "        super(LinkerSizeGNN, self).__init__()\n",
    "        self.num_layers = num_layers \n",
    "        self.input_embedding = nn.Linear(input_node_features, hidden_node_features)\n",
    "        self.output_embedding = nn.Linear(hidden_node_features, output_node_features)\n",
    "\n",
    "        for l in range(num_layers):\n",
    "            self.add_module(f\"LinkerSizeGCL{l}\", LinkerSizeGCL(\n",
    "                input_node_features=hidden_node_features,\n",
    "                hidden_node_features=hidden_node_features,\n",
    "                output_node_features=hidden_node_features,\n",
    "                use_residual=use_residual,\n",
    "                use_normalization=use_normalization,\n",
    "                activation_function=activation_function\n",
    "            ))\n",
    "\n",
    "    def forward(self, a, h, edge_index):\n",
    "        h = self.input_embedding(h)\n",
    "        for l in range(self.num_layers):\n",
    "            h = self._modules[f\"LinkerSizeGCL{l}\"](a, h, edge_index)\n",
    "        h = self.output_embedding(h)\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionNoiseModel(nn.Module):\n",
    "    def __init__(self, input_node_features, hidden_node_features, output_node_features, num_layers=8, use_residual=True, use_normalization=True):\n",
    "        super(DiffusionNoiseModel, self).__init__()\n",
    "        # rh passed first (concatenation of r and h along dim=1, meaning rh.shape[1] = r.shape[1] + h.shape[1] = 3 + h.shape[1])\n",
    "        self.egnn = EGNN(\n",
    "            input_node_features=input_node_features,\n",
    "            hidden_node_features=hidden_node_features,\n",
    "            output_node_features=output_node_features,\n",
    "            num_layers=num_layers,\n",
    "            use_residual=use_residual,\n",
    "            use_normalization=use_normalization\n",
    "        )\n",
    "\n",
    "    def forward(self, rh, u_mask, t, edge_index):\n",
    "        # rh has shape (batch_size, num_nodes, num_features)\n",
    "\n",
    "        # prep the data to be fed into the EGNN\n",
    "        batch_size, num_nodes, input_features = rh.shape[0], rh.shape[1], rh.shape[2]\n",
    "        rh = rh.reshape(batch_size * num_nodes, input_features)\n",
    "        u_mask = u_mask.reshape(batch_size * num_nodes, input_features)\n",
    "        r, h = rh[:, :3], rh[:, 3:]\n",
    "        h = torch.cat([h, t], dim=1)\n",
    "        \n",
    "        # run the EGNN\n",
    "        r, h = self.egnn(r, h, u_mask, edge_index)\n",
    "        # discard the time dimension and context nodes\n",
    "        rh = torch.cat([r, h[:, :-1]], dim=1)[u_mask].reshape(batch_size, num_nodes, -1)\n",
    "        \n",
    "        return rh    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, u_mask, diffusion_model, edge_index, num_epochs=300, learning_rate=2e-5, batch_size=128, s=1e-5, T=500):\n",
    "    # Split the training data into mini-batches\n",
    "    shuffled_indices = torch.randperm(X.shape[0])\n",
    "    X = X[shuffled_indices, :, :]\n",
    "    u_mask = u_mask[shuffled_indices, :, :]\n",
    "    num_training_examples = X.shape[0]\n",
    "    num_batches = np.ceil(num_training_examples / batch_size)\n",
    "    X_batches, u_mask_batches = [], []\n",
    "    for curr_batch_number in range(num_batches):\n",
    "        start_index = curr_batch_number * batch_size\n",
    "        end_index = min((curr_batch_number + 1) * batch_size, num_training_examples)\n",
    "        X_batches.append(X[start_index:end_index, :, :])\n",
    "        u_mask_batches.append(u_mask[start_index:end_index, :, :])\n",
    "    \n",
    "    # Train the diffusion model\n",
    "    curr_epoch_number, batch_index = 0, 0\n",
    "    while(curr_epoch_number < num_epochs):\n",
    "        curr_X_batch = X_batches[batch_index]\n",
    "        curr_u_mask_batch = u_mask_batches[batch_index]\n",
    "        batch_index = (batch_index + 1) % num_training_examples\n",
    "        if(batch_index == 0): \n",
    "            curr_epoch_number += 1\n",
    "        \n",
    "        t = torch.randint(low=0, high=T, size=(batch_size, 1)) # (batch_size, 1)\n",
    "        e_t = torch.randn_like(curr_X_batch)\n",
    "        alpha_t = (1 - 2 * s) * (1 - (t / T) ** 2) # (batch_size, 1)\n",
    "        sigma_t = 1 - alpha_t # (batch_size, 1)\n",
    "        z_t = alpha_t * curr_X_batch + sigma_t * e_t\n",
    "        predicted_e_t = diffusion_model(curr_X_batch, curr_u_mask_batch, t, edge_index)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
