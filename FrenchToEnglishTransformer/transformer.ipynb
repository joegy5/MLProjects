{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #type: ignore\n",
    "import torch.nn as nn #type: ignore\n",
    "import numpy #type: ignore\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS CLASS IS DEFINITELY CORRECTLY IMPLEMENTED (checked with StatQuest)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def forward(self, batch_X):\n",
    "        _, max_sentence_length, d_model = batch_X.shape\n",
    "        \n",
    "        positional_encodings = torch.arange(start=0, end=max_sentence_length, dtype=torch.float32).unsqueeze(-1).expand(-1, d_model).clone() # .expand() doesn't create new memory for the duplicated dimension, it uses shared memory --> clone it to not used shared memory\n",
    "        embedding_dimensions = torch.arange(start=0, end=d_model, step=2, dtype=torch.float32)\n",
    "        div_factor = torch.tensor(10000) ** (embedding_dimensions / d_model)\n",
    "        div_factor = div_factor.float()\n",
    "\n",
    "        positional_encodings[:, 0::2] = torch.sin(positional_encodings[:, 0::2] / div_factor)\n",
    "        positional_encodings[:, 1::2] = torch.cos(positional_encodings[:, 1::2] / div_factor)\n",
    "        return batch_X.float() + positional_encodings.float() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, use_mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.use_mask = use_mask\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads \n",
    "        \n",
    "        self.W_Q = nn.Linear(in_features=d_model, out_features=d_model, dtype=torch.float32, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_Q.weight)\n",
    "        \n",
    "        self.W_K = nn.Linear(in_features=d_model, out_features=d_model, dtype=torch.float32, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_K.weight)\n",
    "        \n",
    "        self.W_V = nn.Linear(in_features=d_model, out_features=d_model, dtype=torch.float32, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_V.weight)\n",
    "        \n",
    "        self.W_O = nn.Linear(in_features=d_model, out_features=d_model, dtype=torch.float32, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_O.weight)\n",
    "\n",
    "    def create_mask(self, batch_size, sentence_length, padding_mask, use_attention_mask):\n",
    "        padding_mask = padding_mask.unsqueeze(-1).expand(-1, sentence_length, sentence_length).float()\n",
    "        if use_attention_mask:\n",
    "            causal_mask = torch.tril(torch.ones(sentence_length, sentence_length, dtype=torch.float32)).unsqueeze(0).expand(batch_size, sentence_length, sentence_length).float()\n",
    "            combined_mask = torch.min(padding_mask, causal_mask).float()\n",
    "        else:\n",
    "            combined_mask = padding_mask.float()\n",
    "        return combined_mask == 0\n",
    "\n",
    "    def forward(self, batch_X, padding_mask, encoder_output=None):\n",
    "        batch_X = batch_X.float()\n",
    "        batch_size, sentence_length, d_model = batch_X.shape\n",
    "        Q = self.W_Q(batch_X).reshape(batch_size, sentence_length, self.num_heads, self.d_k).permute(0, 2, 1, 3).float()\n",
    "        if encoder_output is not None:\n",
    "            K = self.W_K(encoder_output).reshape(batch_size, sentence_length, self.num_heads, self.d_k).permute(0, 2, 1, 3).float()\n",
    "            V = self.W_V(encoder_output).reshape(batch_size, sentence_length, self.num_heads, self.d_k).permute(0, 2, 1, 3).float()\n",
    "        else:\n",
    "            K = self.W_K(batch_X).reshape(batch_size, sentence_length, self.num_heads, self.d_k).permute(0, 2, 1, 3).float()\n",
    "            V = self.W_V(batch_X).reshape(batch_size, sentence_length, self.num_heads, self.d_k).permute(0, 2, 1, 3).float()\n",
    "\n",
    "        # torch.matmul() performs the matrix multiplication over the last 2 dimensions, broadcasting all the others\n",
    "        mask = self.create_mask(batch_size, sentence_length, padding_mask.float(), self.use_mask).unsqueeze(1).expand(batch_size, self.num_heads, sentence_length, sentence_length)\n",
    "        attention_scores = (torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float)))\n",
    "\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf')).float()\n",
    "        scaled_attention_scores = nn.functional.softmax(attention_scores, dim=-1).float()\n",
    "        \n",
    "        all_nan_rows_mask = torch.all(mask, dim=-1, keepdim=True) # (batch_size, num_heads, sentence_length, 1)\n",
    "        scaled_attention_scores = scaled_attention_scores.masked_fill(all_nan_rows_mask, 0.0)\n",
    "        \n",
    "        scaled_dot_product_attention = torch.matmul(scaled_attention_scores, V).float() # shape = (batch_size, num_heads, sentence_length, d_v)\n",
    "        \n",
    "        # Concatenate all the heads\n",
    "        scaled_dot_product_attention = scaled_dot_product_attention.permute(0, 2, 1, 3).reshape(batch_size, sentence_length, d_model).float()\n",
    "        \n",
    "        return self.W_O(scaled_dot_product_attention) # shape = (batch_size, sentence_length, d_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, dropout_rate, activation=nn.ReLU()):\n",
    "        super(FFN, self).__init__()\n",
    "        self.activation_function = activation\n",
    "        d_ff = d_model * 4\n",
    "    \n",
    "        self.linear1 = nn.Linear(in_features=d_model, out_features=d_ff, dtype=torch.float32)\n",
    "        nn.init.kaiming_uniform_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "\n",
    "        self.linear2 = nn.Linear(in_features=d_ff, out_features=d_model, dtype=torch.float32)\n",
    "        nn.init.kaiming_uniform_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, batch_X):\n",
    "        batch_X = self.dropout(self.activation(self.linear1(batch_X.float())))\n",
    "        return self.linear2(batch_X.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.ffn = FFN(d_model=d_model, dropout_rate=dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, dtype=torch.float32, eps=1e-6)\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, dtype=torch.float32, eps=1e-6)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, batch_X, padding_mask):\n",
    "        batch_X = batch_X.float() + self.dropout(self.layer_norm1(self.mha(batch_X, padding_mask)))\n",
    "        batch_X = batch_X.float() + self.dropout(self.layer_norm2(self.ffn(batch_X)))\n",
    "        return batch_X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_layers, num_heads, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model, num_heads=num_heads, dropout_rate=dropout_rate) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, batch_X, padding_mask):\n",
    "        for encoder_layer in self.layers:\n",
    "            batch_X = encoder_layer(batch_X, padding_mask)\n",
    "        return self.layer_norm(batch_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads, use_mask=True)\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, dtype=torch.float32)\n",
    "        self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, dtype=torch.float32)\n",
    "        self.ffn = FFN(d_model=d_model, dropout_rate=dropout_rate)\n",
    "        self.layernorm3 = nn.LayerNorm(normalized_shape=d_model, dtype=torch.float32)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, batch_X, encoder_output, padding_mask):\n",
    "        batch_X = batch_X.float() + self.dropout(self.layernorm1(self.mha1(batch_X, padding_mask)))\n",
    "        batch_X = batch_X.float() + self.dropout(self.layernorm2(self.mha2(batch_X, padding_mask, encoder_output)))\n",
    "        batch_X = batch_X.float() + self.dropout(self.layernorm3(self.ffn(batch_X)))\n",
    "        return batch_X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_layers, num_heads, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model, num_heads=num_heads, dropout_rate=dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, batch_X, encoder_output, padding_mask):\n",
    "        for decoder_layer in self.layers:\n",
    "            batch_X = decoder_layer(batch_X.float(), encoder_output, padding_mask)\n",
    "        return batch_X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, dtype=torch.float32)\n",
    "        self.encoder = Encoder(\n",
    "            d_model=d_model, \n",
    "            num_layers=num_layers, \n",
    "            num_heads=num_heads, \n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        \n",
    "        self.decoder_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, dtype=torch.float32)\n",
    "        self.decoder = Decoder(\n",
    "            d_model=d_model, \n",
    "            num_layers=num_layers, \n",
    "            num_heads=num_heads, \n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size, dtype=torch.float32)\n",
    "        nn.init.xavier_uniform_(self.linear.weight, gain=0.1)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, encoder_input, shifted_decoder_input, encoder_padding_masks, decoder_padding_masks):\n",
    "        embedded_encoder_input = self.encoder_embedding(encoder_input).float()\n",
    "        embedded_decoder_input = self.decoder_embedding(shifted_decoder_input).float()\n",
    "        \n",
    "        encoder_output = self.encoder(\n",
    "            self.positional_encoding(embedded_encoder_input),\n",
    "            encoder_padding_masks\n",
    "        ).float()\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            self.positional_encoding(embedded_decoder_input),\n",
    "            encoder_output, \n",
    "            decoder_padding_masks\n",
    "        ).float()\n",
    "        \n",
    "        logits = self.linear(decoder_output).float()\n",
    "        output_probabilities = self.softmax(logits).float()\n",
    "        \n",
    "        return output_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerLoss, self).__init__()\n",
    "\n",
    "    def forward(self, decoder_output, target_sequences, padding_vocab_index):\n",
    "        # decoder_output has shape (batch_size, sentence_length, vocab_size)\n",
    "        # target_sequences has shape (batch_size, sentence_length)\n",
    "        # for each training example, each of the vocab_size positions in each row\n",
    "            # has a corresponding probability of being selected, and each corresponding row in the target\n",
    "            # will have a value equal to the correct position representing a word in the vocabulary\n",
    "        # print(target_sequences)\n",
    "        batch_size, sentence_length, vocab_size = decoder_output.shape\n",
    "\n",
    "        flattened_decoder_output = decoder_output.reshape(batch_size * sentence_length, vocab_size) \n",
    "        flattened_target_sequences = target_sequences.reshape(batch_size * sentence_length)\n",
    "        \n",
    "        # print(\"Decoder Output Shape:\", decoder_output.shape)\n",
    "        # print(\"Target Sequences Shape:\", target_sequences.shape)\n",
    "        # print(\"Flattened Decoder Output Shape:\", flattened_decoder_output.shape)\n",
    "        # print(\"Flattened Target Sequences Shape:\", flattened_target_sequences.shape)\n",
    "\n",
    "\n",
    "        return nn.functional.cross_entropy(input=flattened_decoder_output, \n",
    "                                           target=flattened_target_sequences, \n",
    "                                           reduction='mean',\n",
    "                                           ignore_index=padding_vocab_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer # type: ignore \n",
    "from datasets import load_dataset # type: ignore\n",
    "\n",
    "wmt_dataset = load_dataset('iwslt2017', 'iwslt2017-fr-en')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def tokenize(examples):\n",
    "    english_examples = [example['en'] for example in examples['translation']]\n",
    "    french_examples = [example['fr'] for example in examples['translation']]\n",
    "    \n",
    "    english_examples = tokenizer(english_examples, padding='max_length', truncation=True, max_length=128)\n",
    "    french_examples = tokenizer(french_examples, padding='max_length', truncation=True, max_length=128)\n",
    "    return {\n",
    "        # all of these should have shape (batch_size, max_length)\n",
    "        'input_token_ids': french_examples['input_ids'], \n",
    "        'encoder_attention_mask': french_examples['attention_mask'], # mask for padded sequences\n",
    "        'decoder_attention_mask': english_examples['attention_mask'],\n",
    "        'labels': english_examples['input_ids']\n",
    "    }\n",
    "\n",
    "tokenized_datasets = wmt_dataset.map(tokenize, batched=True)\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 6, 1872, 220, 25125, 2634, 491, 14064, 82, 10647, 77, 2634, 1582, 269, 5857, 1013, 2634, 6784, 11, 2123, 11223, 256, 10465, 28141, 410, 516, 302, 647, 66, 959, 256, 516, 12797, 410, 418, 299, 2381, 260, 2821, 2123, 23860, 6368, 2912, 17693, 969, 2906, 8358, 474, 6, 1872, 288, 270, 300, 6, 2306, 260, 523, 343, 13, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['input_token_ids'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'ai été très impressionné par cette conférence, et je tiens à vous remercier tous pour vos nombreux et sympathiques commentaires sur ce que j'ai dit l'autre soir.[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_datasets['train']['input_token_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_datasets['train']['labels'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 1 # add 1 because you manually added padding token, increasing vocab size\n",
    "BATCH_SIZE = 1\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "DROPOUT_RATE = 0.1\n",
    "NUM_WORKERS = 8\n",
    "PREFETCH_FACTOR = 2\n",
    "PERSISTENT_WORKERS = True\n",
    "SHUFFLE = True\n",
    "NUM_EPOCHS = 15\n",
    "WARMUP_STEPS = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_lambda_function(step_number):\n",
    "    return (D_MODEL ** (-0.5)) * min((step_number + 1) ** (-0.5), (step_number + 1) * (WARMUP_STEPS ** (-1.5)))\n",
    "\n",
    "def collate_function(batch):\n",
    "    input_ids = torch.tensor([example['input_token_ids'] for example in batch])\n",
    "    encoder_attention_masks = torch.tensor([example['encoder_attention_mask'] for example in batch])\n",
    "    decoder_attention_masks = torch.tensor([example['decoder_attention_mask'] for example in batch])\n",
    "    output_labels = torch.tensor([example['labels'] for example in batch])\n",
    "\n",
    "    return {\n",
    "        'input_token_ids': input_ids,\n",
    "        'encoder_attention_masks': encoder_attention_masks,\n",
    "        'decoder_attention_masks': decoder_attention_masks,\n",
    "        'output_labels': output_labels\n",
    "    }\n",
    "\n",
    "def generate_overfit(model, encoder_inputs, encoder_padding_masks, start_token, max_length, device):\n",
    "    model.eval()\n",
    "    decoder_inputs = torch.tensor([start_token]).unsqueeze(0).expand(batch_size) # (batch_size, sentence_length=1)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            batch_size = encoder_inputs.shape[0]\n",
    "            curr_sentence_length = decoder_inputs.shape[1]\n",
    "            decoder_padding_masks = torch.ones(batch_size, curr_sentence_length)\n",
    "            outputs = model(encoder_inputs, decoder_inputs, encoder_padding_masks, decoder_padding_masks) # (batch_size, sentence_length, vocab_size)\n",
    "            next_tokens = torch.argmax(outputs, dim=-1)[:, -1].unsqueeze(-1) # argmax returns index of largest probability, which is exactly what we want --> (batch_size, sentence_length) --> (batch_size, 1))\n",
    "            decoder_inputs = torch.cat([decoder_inputs, next_tokens], dim=-1)\n",
    "\n",
    "    return decoder_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT EPOCH: 0\n",
      "Loss value: 10.824926376342773\n",
      "CURRENT EPOCH: 1\n",
      "Loss value: 10.824921607971191\n",
      "CURRENT EPOCH: 2\n",
      "Loss value: 10.824841499328613\n",
      "CURRENT EPOCH: 3\n",
      "Loss value: 10.824027061462402\n",
      "CURRENT EPOCH: 4\n",
      "Loss value: 10.815892219543457\n",
      "CURRENT EPOCH: 5\n",
      "Loss value: 10.785075187683105\n",
      "CURRENT EPOCH: 6\n",
      "Loss value: 10.766366958618164\n",
      "CURRENT EPOCH: 7\n",
      "Loss value: 10.743791580200195\n",
      "CURRENT EPOCH: 8\n",
      "Loss value: 10.723159790039062\n",
      "CURRENT EPOCH: 9\n",
      "Loss value: 10.716070175170898\n",
      "CURRENT EPOCH: 10\n",
      "Loss value: 10.692280769348145\n",
      "CURRENT EPOCH: 11\n",
      "Loss value: 10.654926300048828\n",
      "CURRENT EPOCH: 12\n",
      "Loss value: 10.60863208770752\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(decoder_outputs, output_labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_norm \u001b[38;5;241m<\u001b[39m MIN_GRAD_NORM:\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader # type: ignore\n",
    "from torch.optim import Adam # type: ignore\n",
    "from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "model = Transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=256,  # Reduced from 512\n",
    "    num_heads=8,\n",
    "    num_layers=3,  # Reduced from 6\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.float() # convert all model parameters to torch.float32\n",
    "loss_function = TransformerLoss()\n",
    "parameters = model.parameters()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "#scheduler = LambdaLR(optimizer, learning_rate_lambda_function)\n",
    "\n",
    "small_loader = DataLoader(\n",
    "    tokenized_datasets['train'].select(range(1)),  # Just one example\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_function\n",
    ")\n",
    "  \n",
    "MIN_GRAD_NORM = 1e-5\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    print(\"CURRENT EPOCH: \" + str(epoch))\n",
    "    for batch_index, batch in enumerate(small_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move everything to device and set correct dtypes\n",
    "        input_token_ids = batch['input_token_ids'].long().to(DEVICE)\n",
    "        encoder_padding_masks = batch['encoder_attention_masks'].to(DEVICE)\n",
    "        decoder_padding_masks = batch['decoder_attention_masks'].to(DEVICE)\n",
    "        output_labels = batch['output_labels'].long().to(DEVICE)\n",
    "\n",
    "        # Create start tokens\n",
    "        start_token_batch = torch.full((BATCH_SIZE, 1), \n",
    "                                     tokenizer.bos_token_id,\n",
    "                                     dtype=torch.long,\n",
    "                                     device=DEVICE)\n",
    "        shifted_output_labels = torch.cat([start_token_batch, output_labels[:, :-1]], dim=-1)\n",
    "\n",
    "        decoder_outputs = model(input_token_ids, shifted_output_labels, encoder_padding_masks, decoder_padding_masks)\n",
    "        # Compute loss\n",
    "        loss = loss_function(decoder_outputs, output_labels, tokenizer.pad_token_id)\n",
    "        print(f\"Loss value: {loss.item()}\")\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        if grad_norm < MIN_GRAD_NORM:\n",
    "            scale_factor = MIN_GRAD_NORM / grad_norm\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.mul_(scale_factor)\n",
    "\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     33\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_token_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_token_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:346\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    344\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:1083\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m   1080\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m   1081\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1083\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:1015\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m   1013\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m-> 1015\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=SHUFFLE, \n",
    "    collate_fn=collate_function, \n",
    "    pin_memory=PIN_MEMORY, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=PERSISTENT_WORKERS\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=SHUFFLE, \n",
    "    collate_fn=collate_function, \n",
    "    pin_memory=PIN_MEMORY, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=PERSISTENT_WORKERS\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=SHUFFLE, \n",
    "    collate_fn=collate_function, \n",
    "    pin_memory=PIN_MEMORY, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=PERSISTENT_WORKERS\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_token_ids = batch['input_token_ids'].to(DEVICE)\n",
    "        attention_masks = batch['attention_masks'].to(DEVICE)\n",
    "        output_labels = batch['output_labels'].to(DEVICE)\n",
    "\n",
    "        decoder_outputs = model(input_token_ids, output_labels[:, :-1], attention_masks)\n",
    "        loss = loss_function(decoder_outputs, output_labels)\n",
    "        print(\"training batch loss: \" + str(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
