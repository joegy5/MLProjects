{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #type: ignore\n",
    "import torch.nn as nn #type: ignore\n",
    "import numpy #type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(start=0, end=6, step=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def forward(self, batch_X):\n",
    "        _, max_sentence_length, d_model = batch_X.shape\n",
    "        \n",
    "        positional_encodings = torch.arange(start=0, end=max_sentence_length).unsqueeze(-1).expand(-1, d_model).clone() # .expand() doesn't create new memory for the duplicated dimension, it uses shared memory --> clone it to not used shared memory\n",
    "        embedding_dimensions = torch.arange(start=0, end=d_model, step=2)\n",
    "        positional_encodings[:, 0::2] = torch.sin(positional_encodings[:, 0::2] / (10000 ** (embedding_dimensions / d_model)))\n",
    "        positional_encodings[:, 1::2] = torch.cos(positional_encodings[:, 1::2] / (10000 ** (embedding_dimensions / d_model)))\n",
    "\n",
    "        return batch_X + positional_encodings # broadcasting so that positional_encodings added to every training example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, use_mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Broadcasting --> don't need to worry about batch dimension\n",
    "    \n",
    "        # nn.Linear matrix has shape (out_features, in_features), performs computation XW\n",
    "        self.use_mask = use_mask\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads \n",
    "        self.d_v = self.d_k\n",
    "        self.W_Q = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.W_K = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.W_V = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.W_O = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "\n",
    "    def create_mask(self, batch_size, sentence_length, padding_mask, use_attention_mask):\n",
    "        padding_mask = padding_mask.unsqueeze(-1).expand(-1, sentence_length, sentence_length)\n",
    "        if use_attention_mask:\n",
    "            causal_mask = torch.tril(torch.ones(sentence_length, sentence_length)).unsqueeze(0).expand(batch_size, sentence_length, sentence_length)\n",
    "            combined_mask = torch.min(padding_mask, causal_mask)\n",
    "        else:\n",
    "            combined_mask = padding_mask\n",
    "        return combined_mask == 0\n",
    "\n",
    "    def forward(self, batch_X, padding_mask, dropout_rate, encoder_output=None):\n",
    "        batch_size, sentence_length, d_model = batch_X.shape\n",
    "        \n",
    "        Q = None\n",
    "        if encoder_output is not None:\n",
    "            Q = self.W_Q(encoder_output).permute(0, 2, 1).reshape(batch_size, sentence_length, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            Q = self.W_Q(batch_X).permute(0, 2, 1).reshape(batch_size, sentence_length, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        K = self.W_K(batch_X).permute(0, 2, 1).reshape(batch_size, sentence_length, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        V = self.W_V(batch_X).permute(0, 2, 1).reshape(batch_size, sentence_length, self.num_heads, self.d_v).permute(0, 2, 1, 3)\n",
    "\n",
    "        # torch.matmul() performs the matrix multiplication over the last 2 dimensions, broadcasting all the others\n",
    "        mask = self.create_mask(batch_size, sentence_length, padding_mask, self.use_mask).unsqueeze(1).expand(batch_size, self.num_heads, sentence_length, sentence_length)\n",
    "        attention_scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float)).masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        scaled_attention_scores = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        #scaled_dot_product_attention = nn.Dropout(dropout_rate)(torch.matmul(scaled_attention_scores, V)) # shape = (batch_size, num_heads, sentence_length, d_v)\n",
    "        scaled_dot_product_attention = torch.matmul(scaled_attention_scores, V) # shape = (batch_size, num_heads, sentence_length, d_v)\n",
    "        \n",
    "        # Concatenate all the heads\n",
    "        scaled_dot_product_attention = scaled_dot_product_attention.permute(0, 2, 1, 3).reshape(batch_size, sentence_length, d_model)\n",
    "        \n",
    "        return self.W_O(scaled_dot_product_attention) # shape = (batch_size, sentence_length, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, activation=nn.ReLU()):\n",
    "        super(FFN, self).__init__()\n",
    "        d_ff = d_model * 4\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model, out_features=d_ff),\n",
    "            activation,\n",
    "            nn.Linear(in_features=d_ff, out_features=d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_X):\n",
    "        return self.ffn(batch_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.ffn = FFN(d_model=d_model)\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model)\n",
    "\n",
    "    def forward(self, batch_X, padding_mask, dropout_rate):\n",
    "        # print(\"input shape pre mha: \" + str(batch_X.shape))\n",
    "        batch_X = self.layer_norm1(batch_X + self.mha(batch_X, padding_mask, dropout_rate))\n",
    "        # print(\"input post mha: \" + str(batch_X.shape))\n",
    "        #batch_X = self.layer_norm2(nn.Dropout(dropout_rate)(batch_X + self.ffn(batch_X)))\n",
    "        batch_X = self.layer_norm2(batch_X + self.ffn(batch_X))\n",
    "        return batch_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_layers, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        layer_list= [EncoderLayer(d_model=d_model, num_heads=num_heads) for l in range(num_layers)]\n",
    "        for l in range(num_layers):\n",
    "            self.add_module(f\"EncoderLayer{l}\", layer_list[l])\n",
    "\n",
    "    def forward(self, batch_X, padding_mask, dropout_rate):\n",
    "        for l in range(self.num_layers):\n",
    "            batch_X = self._modules[f\"EncoderLayer{l}\"](batch_X, padding_mask, dropout_rate)\n",
    "        return batch_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads, use_mask=True)\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        self.ffn = FFN(d_model=d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(normalized_shape=d_model)\n",
    "\n",
    "    def forward(self, batch_X, encoder_output, padding_mask, dropout_rate):\n",
    "        batch_X = self.layernorm1(batch_X + self.mha1(batch_X, padding_mask, dropout_rate))\n",
    "        batch_X = self.layernorm2(batch_X + self.mha2(batch_X, padding_mask, dropout_rate, encoder_output))\n",
    "        \n",
    "        #batch_X = self.layernorm3(nn.Dropout(dropout_rate)(batch_X + self.ffn(batch_X)))\n",
    "        batch_X = self.layernorm3(batch_X + self.ffn(batch_X))\n",
    "        return batch_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_layers, num_heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        layer_list = [DecoderLayer(d_model=d_model, num_heads=num_heads) for l in range(num_layers)]\n",
    "        for l in range(num_layers):\n",
    "            self.add_module(f\"DecoderLayer{l}\", layer_list[l])\n",
    "\n",
    "    def forward(self, batch_X, encoder_output, padding_mask, dropout_rate):\n",
    "        for l in range(self.num_layers):\n",
    "            batch_X = self._modules[f\"DecoderLayer{l}\"](batch_X, encoder_output, padding_mask, dropout_rate)\n",
    "        return batch_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.encoder_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.encoder_embedding_dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder = Encoder(d_model=d_model, num_layers=num_layers, num_heads=num_heads)\n",
    "        self.decoder_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.decoder_embedding_dropout = nn.Dropout(dropout_rate)\n",
    "        self.decoder = Decoder(d_model=d_model, num_layers=num_layers, num_heads=num_heads)\n",
    "        self.decoder_dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, encoder_input, shifted_decoder_input, encoder_padding_masks, decoder_padding_masks):\n",
    "        # embedded_encoder_input = self.encoder_embedding_dropout(self.encoder_embedding(encoder_input))\n",
    "        embedded_encoder_input = self.encoder_embedding(encoder_input)\n",
    "        # print(\"embedded encoder input: \" + str(embedded_encoder_input))\n",
    "        # embedded_decoder_input = self.decoder_embedding_dropout(self.decoder_embedding(shifted_decoder_input))\n",
    "        embedded_decoder_input = self.decoder_embedding(shifted_decoder_input)\n",
    "        # print(\"embedded decoder input: \" + str(embedded_decoder_input))\n",
    "        encoder_output = self.positional_encoding(embedded_encoder_input)\n",
    "        # print(\"encoder positional_encodings: \" + str(encoder_output))\n",
    "        encoder_output = self.encoder(encoder_output, encoder_padding_masks, self.dropout_rate)\n",
    "        # print(\"encoder output: \" + str(encoder_output))\n",
    "        decoder_output = self.decoder(self.positional_encoding(embedded_decoder_input), encoder_output, decoder_padding_masks, self.dropout_rate)\n",
    "        # print(\"decoder_output: \" + str(decoder_output))\n",
    "        output_probabilities = self.softmax(self.decoder_dropout(self.linear(decoder_output)))\n",
    "        # print(\"output probabilities: \" + str(output_probabilities))\n",
    "        return output_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerLoss, self).__init__()\n",
    "\n",
    "    def forward(self, decoder_output, target_sequences, padding_vocab_index):\n",
    "        # decoder_output has shape (batch_size, sentence_length, vocab_size)\n",
    "        # target_sequences has shape (batch_size, sentence_length)\n",
    "        # for each training example, each of the vocab_size positions in each row\n",
    "            # has a corresponding probability of being selected, and each corresponding row in the target\n",
    "            # will have a value equal to the correct position representing a word in the vocabulary\n",
    "        # print(target_sequences)\n",
    "        batch_size, sentence_length, vocab_size = decoder_output.shape\n",
    "        flattened_decoder_output = decoder_output.reshape(batch_size * sentence_length, vocab_size) \n",
    "        flattened_target_sequences = target_sequences.reshape(batch_size * sentence_length)\n",
    "        \n",
    "        return nn.functional.cross_entropy(input=flattened_decoder_output, \n",
    "                                           target=flattened_target_sequences, \n",
    "                                           reduction='mean',\n",
    "                                           ignore_index=padding_vocab_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer # type: ignore \n",
    "from datasets import load_dataset # type: ignore\n",
    "\n",
    "wmt_dataset = load_dataset('iwslt2017', 'iwslt2017-fr-en')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def tokenize(examples):\n",
    "    english_examples = [example['en'] for example in examples['translation']]\n",
    "    french_examples = [example['fr'] for example in examples['translation']]\n",
    "    \n",
    "    english_examples = tokenizer(english_examples, padding='max_length', truncation=True, max_length=128)\n",
    "    french_examples = tokenizer(french_examples, padding='max_length', truncation=True, max_length=128)\n",
    "    return {\n",
    "        # all of these should have shape (batch_size, max_length)\n",
    "        'input_token_ids': french_examples['input_ids'], \n",
    "        'encoder_attention_mask': french_examples['attention_mask'], # mask for padded sequences\n",
    "        'decoder_attention_mask': english_examples['attention_mask'],\n",
    "        'labels': english_examples['input_ids']\n",
    "    }\n",
    "\n",
    "tokenized_datasets = wmt_dataset.map(tokenize, batched=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 1 # add 1 because you manually added padding token, increasing vocab size\n",
    "BATCH_SIZE = 64\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "DROPOUT_RATE = 0.1\n",
    "NUM_WORKERS = 8\n",
    "PREFETCH_FACTOR = 2\n",
    "PERSISTENT_WORKERS = True\n",
    "SHUFFLE = True\n",
    "NUM_EPOCHS = 15\n",
    "WARMUP_STEPS = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor([[1., 2., 3.], [1., 2., 3.]])\n",
    "test = nn.functional.softmax(test, dim=-1)\n",
    "print(test)\n",
    "test = torch.argmax(test, dim=-1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_lambda_function(step_number):\n",
    "    return D_MODEL ** (-0.5) * min((step_number + 1) ** (-0.5), (step_number + 1) * WARMUP_STEPS ** (-1.5))\n",
    "\n",
    "def collate_function(batch):\n",
    "    input_ids = torch.stack([torch.tensor(example['input_token_ids']) for example in batch])\n",
    "    encoder_attention_masks = torch.stack([torch.tensor(example['encoder_attention_mask']) for example in batch])\n",
    "    decoder_attention_masks = torch.stack([torch.tensor(example['decoder_attention_mask']) for example in batch])\n",
    "    output_labels = torch.stack([torch.tensor(example['labels']) for example in batch])\n",
    "\n",
    "    return {\n",
    "        'input_token_ids': input_ids,\n",
    "        'encoder_attention_masks': encoder_attention_masks,\n",
    "        'decoder_attention_masks': decoder_attention_masks,\n",
    "        'output_labels': output_labels\n",
    "    }\n",
    "\n",
    "def generate_overfit(model, encoder_inputs, encoder_padding_masks, start_token, max_length, device):\n",
    "    model.eval()\n",
    "    decoder_inputs = torch.tensor([start_token]).unsqueeze(0).expand(batch_size) # (batch_size, sentence_length=1)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            batch_size = encoder_inputs.shape[0]\n",
    "            curr_sentence_length = decoder_inputs.shape[1]\n",
    "            decoder_padding_masks = torch.ones(batch_size, curr_sentence_length)\n",
    "            outputs = model(encoder_inputs, decoder_inputs, encoder_padding_masks, decoder_padding_masks) # (batch_size, sentence_length, vocab_size)\n",
    "            next_tokens = torch.argmax(outputs, dim=-1)[:, -1].unsqueeze(-1) # argmax returns index of largest probability, which is exactly what we want --> (batch_size, sentence_length) --> (batch_size, 1))\n",
    "            decoder_inputs = torch.cat([decoder_inputs, next_tokens], dim=-1)\n",
    "\n",
    "    return decoder_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'config' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      3\u001b[0m m \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:78\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     67\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     68\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     77\u001b[0m )\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:371\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:53\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     _disable_current_modes,\n\u001b[0;32m     49\u001b[0m     is_in_torch_dispatch_mode,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traceback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CapturedTraceback, format_traceback_short\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, exc, trace_rules\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_dead_code, remove_pointless_jumps\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     56\u001b[0m     check_inst_exn_tab_entries_valid,\n\u001b[0;32m     57\u001b[0m     Instruction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     transform_code_object,\n\u001b[0;32m     61\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:3218\u001b[0m\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m   3206\u001b[0m     LEGACY_MOD_INLINELIST \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed.tensor._api\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed.tensor.device_mesh\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed._composable.replicate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3217\u001b[0m     }\n\u001b[1;32m-> 3218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mskip_fsdp_hooks:\n\u001b[0;32m   3219\u001b[0m         LEGACY_MOD_INLINELIST\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed._composable.fsdp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3222\u001b[0m \u001b[38;5;66;03m# Force inline functions under these modules, even they are in *_SKIPLIST.\u001b[39;00m\n\u001b[0;32m   3223\u001b[0m \u001b[38;5;66;03m# We are using python module name instead of file or directory object to avoid circular dependency.\u001b[39;00m\n\u001b[0;32m   3224\u001b[0m \u001b[38;5;66;03m# Please keep this sorted alphabetically.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'config' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "m = nn.Linear(in_features=10, out_features=10)\n",
    "opt = Adam(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'config' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m TransformerLoss()\n\u001b[0;32m     15\u001b[0m parameters \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m---> 16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m LambdaLR(optimizer, learning_rate_lambda_function)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# First overfit to small dataset to ensure that the model is working\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:78\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     67\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     68\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     77\u001b[0m )\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:371\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:53\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     _disable_current_modes,\n\u001b[0;32m     49\u001b[0m     is_in_torch_dispatch_mode,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traceback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CapturedTraceback, format_traceback_short\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, exc, trace_rules\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_dead_code, remove_pointless_jumps\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     56\u001b[0m     check_inst_exn_tab_entries_valid,\n\u001b[0;32m     57\u001b[0m     Instruction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     transform_code_object,\n\u001b[0;32m     61\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:3218\u001b[0m\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m   3206\u001b[0m     LEGACY_MOD_INLINELIST \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed.tensor._api\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed.tensor.device_mesh\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed._composable.replicate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3217\u001b[0m     }\n\u001b[1;32m-> 3218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mskip_fsdp_hooks:\n\u001b[0;32m   3219\u001b[0m         LEGACY_MOD_INLINELIST\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed._composable.fsdp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3222\u001b[0m \u001b[38;5;66;03m# Force inline functions under these modules, even they are in *_SKIPLIST.\u001b[39;00m\n\u001b[0;32m   3223\u001b[0m \u001b[38;5;66;03m# We are using python module name instead of file or directory object to avoid circular dependency.\u001b[39;00m\n\u001b[0;32m   3224\u001b[0m \u001b[38;5;66;03m# Please keep this sorted alphabetically.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'config' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader # type: ignore\n",
    "from torch.optim import Adam # type: ignore\n",
    "from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "model = Transformer(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    d_model=D_MODEL, \n",
    "    num_heads=NUM_HEADS, \n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "model.to(DEVICE)\n",
    "loss_function = TransformerLoss()\n",
    "parameters = model.parameters()\n",
    "optimizer = Adam(parameters, lr=1e-4)\n",
    "scheduler = LambdaLR(optimizer, learning_rate_lambda_function)\n",
    "\n",
    "# First overfit to small dataset to ensure that the model is working\n",
    "small_loader = DataLoader(\n",
    "    tokenized_datasets['train'].select(range(128)), # select first 128 examples\n",
    "    batch_size=BATCH_SIZE, \n",
    "    #shuffle=SHUFFLE, \n",
    "    collate_fn=collate_function, \n",
    "    #pin_memory=PIN_MEMORY, \n",
    "    #num_workers=NUM_WORKERS,\n",
    "    #prefetch_factor=PREFETCH_FACTOR,\n",
    "    #persistent_workers=PERSISTENT_WORKERS\n",
    ")\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    print(\"CURRENT EPOCH: \" + str(epoch))\n",
    "    for batch_index, batch in enumerate(small_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_token_ids = batch['input_token_ids'].to(DEVICE)\n",
    "        encoder_padding_masks = batch['encoder_attention_masks'].to(DEVICE)\n",
    "        decoder_padding_masks = batch['decoder_attention_masks'].to(DEVICE)\n",
    "        output_labels = batch['output_labels'].to(DEVICE)\n",
    "\n",
    "        start_token_batch = torch.full((BATCH_SIZE, 1), tokenizer.bos_token_id)\n",
    "        shifted_output_labels = torch.cat([start_token_batch, output_labels[:, :-1]], dim=-1)\n",
    "\n",
    "        decoder_outputs = model(input_token_ids, shifted_output_labels, encoder_padding_masks, decoder_padding_masks)\n",
    "        loss = loss_function(decoder_outputs, output_labels, tokenizer.pad_token_id)\n",
    "        print(\"overfit batch loss: \" + str(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# for batch_index, batch in enumerate(small_loader):\n",
    "#     encoder_inputs = batch['input_token_ids']\n",
    "#     start_token = tokenizer.bos_token_id\n",
    "#     max_length = batch.shape[1]\n",
    "#     device = DEVICE\n",
    "#     generate_overfit(model, encoder_inputs, encoder_padding_masks, start_token, max_length, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=SHUFFLE, \n",
    "    collate_fn=collate_function, \n",
    "    pin_memory=PIN_MEMORY, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=PERSISTENT_WORKERS\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=SHUFFLE, \n",
    "    collate_fn=collate_function, \n",
    "    pin_memory=PIN_MEMORY, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=PERSISTENT_WORKERS\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=SHUFFLE, \n",
    "    collate_fn=collate_function, \n",
    "    pin_memory=PIN_MEMORY, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=PERSISTENT_WORKERS\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_token_ids = batch['input_token_ids'].to(DEVICE)\n",
    "        attention_masks = batch['attention_masks'].to(DEVICE)\n",
    "        output_labels = batch['output_labels'].to(DEVICE)\n",
    "\n",
    "        decoder_outputs = model(input_token_ids, output_labels[:, :-1], attention_masks)\n",
    "        loss = loss_function(decoder_outputs, output_labels)\n",
    "        print(\"training batch loss: \" + str(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
