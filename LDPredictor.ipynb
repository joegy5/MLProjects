{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ID  year loan_limit             Gender approv_in_adv loan_type  \\\n",
      "0        24890  2019         cf  Sex Not Available         nopre     type1   \n",
      "1        24891  2019         cf               Male         nopre     type2   \n",
      "2        24892  2019         cf               Male           pre     type1   \n",
      "3        24893  2019         cf               Male         nopre     type1   \n",
      "4        24894  2019         cf              Joint           pre     type1   \n",
      "...        ...   ...        ...                ...           ...       ...   \n",
      "148665  173555  2019         cf  Sex Not Available         nopre     type1   \n",
      "148666  173556  2019         cf               Male         nopre     type1   \n",
      "148667  173557  2019         cf               Male         nopre     type1   \n",
      "148668  173558  2019         cf             Female         nopre     type1   \n",
      "148669  173559  2019         cf             Female         nopre     type1   \n",
      "\n",
      "       loan_purpose Credit_Worthiness open_credit business_or_commercial  ...  \\\n",
      "0                p1                l1        nopc                  nob/c  ...   \n",
      "1                p1                l1        nopc                    b/c  ...   \n",
      "2                p1                l1        nopc                  nob/c  ...   \n",
      "3                p4                l1        nopc                  nob/c  ...   \n",
      "4                p1                l1        nopc                  nob/c  ...   \n",
      "...             ...               ...         ...                    ...  ...   \n",
      "148665           p3                l1        nopc                  nob/c  ...   \n",
      "148666           p1                l1        nopc                  nob/c  ...   \n",
      "148667           p4                l1        nopc                  nob/c  ...   \n",
      "148668           p4                l1        nopc                  nob/c  ...   \n",
      "148669           p3                l1        nopc                  nob/c  ...   \n",
      "\n",
      "        credit_type  Credit_Score  co-applicant_credit_type    age  \\\n",
      "0               EXP           758                       CIB  25-34   \n",
      "1              EQUI           552                       EXP  55-64   \n",
      "2               EXP           834                       CIB  35-44   \n",
      "3               EXP           587                       CIB  45-54   \n",
      "4              CRIF           602                       EXP  25-34   \n",
      "...             ...           ...                       ...    ...   \n",
      "148665          CIB           659                       EXP  55-64   \n",
      "148666          CIB           569                       CIB  25-34   \n",
      "148667          CIB           702                       EXP  45-54   \n",
      "148668          EXP           737                       EXP  55-64   \n",
      "148669          CIB           830                       CIB  45-54   \n",
      "\n",
      "        submission_of_application        LTV Region Security_Type  Status  \\\n",
      "0                         to_inst  98.728814  south        direct       1   \n",
      "1                         to_inst        NaN  North        direct       1   \n",
      "2                         to_inst  80.019685  south        direct       0   \n",
      "3                        not_inst  69.376900  North        direct       0   \n",
      "4                        not_inst  91.886544  North        direct       0   \n",
      "...                           ...        ...    ...           ...     ...   \n",
      "148665                    to_inst  71.792763  south        direct       0   \n",
      "148666                   not_inst  74.428934  south        direct       0   \n",
      "148667                   not_inst  61.332418  North        direct       0   \n",
      "148668                    to_inst  70.683453  North        direct       0   \n",
      "148669                   not_inst  72.849462  North        direct       0   \n",
      "\n",
      "       dtir1  \n",
      "0       45.0  \n",
      "1        NaN  \n",
      "2       46.0  \n",
      "3       42.0  \n",
      "4       39.0  \n",
      "...      ...  \n",
      "148665  48.0  \n",
      "148666  15.0  \n",
      "148667  49.0  \n",
      "148668  29.0  \n",
      "148669  44.0  \n",
      "\n",
      "[148670 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\joelp\\\\Downloads\\\\LoanDefaultDataset\\\\Loan_Default.csv')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148670, 113)\n",
      "ID                         24890.000000\n",
      "year                        2019.000000\n",
      "loan_amount               116500.000000\n",
      "rate_of_interest               4.045476\n",
      "Interest_rate_spread           0.441656\n",
      "                              ...      \n",
      "Region_North-East              0.000000\n",
      "Region_central                 0.000000\n",
      "Region_south                   1.000000\n",
      "Security_Type_Indriect         0.000000\n",
      "Security_Type_direct           1.000000\n",
      "Name: 0, Length: 113, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create missing indicator for each row that has missing data, and also replace the missing value with the mean of the column's values\n",
    "# Also one_hot_encode the dataset\n",
    "\n",
    "# df.isnull() returns a Series containing values True or False. Taking their sum will return positive number if there is at least one NaN value\n",
    "columns_with_missing_data = df.columns[df.isnull().sum() > 0]\n",
    "for column in columns_with_missing_data:\n",
    "    df[f\"{column}_missing\"] = df[column].isnull().astype(int)\n",
    "    # If column is numeric, use mean imputation\n",
    "    if pd.api.types.is_numeric_dtype(df[column]):\n",
    "        df[column].fillna(value=df[column].mean(), inplace=True)\n",
    "\n",
    "columns_to_one_hot_encode = []\n",
    "for column in df.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "        columns_to_one_hot_encode.append(column)\n",
    "        df\n",
    "        \n",
    "\n",
    "df_dummies = pd.get_dummies(data=df, columns=columns_to_one_hot_encode, drop_first=False, dtype=int)\n",
    "df.drop(labels=columns_to_one_hot_encode, axis=1, inplace=True)\n",
    "df = pd.concat([df, df_dummies], axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148670, 109)\n",
      "   loan_amount  rate_of_interest  Interest_rate_spread  Upfront_charges  \\\n",
      "0       116500          4.045476              0.441656      3224.996127   \n",
      "1       206500          4.045476              0.441656      3224.996127   \n",
      "2       406500          4.560000              0.200000       595.000000   \n",
      "3       456500          4.250000              0.681000      3224.996127   \n",
      "4       696500          4.000000              0.304200         0.000000   \n",
      "\n",
      "    term  property_value   income  Credit_Score        LTV  Status  ...  \\\n",
      "0  360.0   118000.000000   1740.0           758  98.728814       1  ...   \n",
      "1  360.0   497893.465696   4980.0           552  72.746457       1  ...   \n",
      "2  360.0   508000.000000   9480.0           834  80.019685       0  ...   \n",
      "3  360.0   658000.000000  11880.0           587  69.376900       0  ...   \n",
      "4  360.0   758000.000000  10440.0           602  91.886544       0  ...   \n",
      "\n",
      "   age_<25  age_>74  submission_of_application_not_inst  \\\n",
      "0        0        0                                   0   \n",
      "1        0        0                                   0   \n",
      "2        0        0                                   0   \n",
      "3        0        0                                   1   \n",
      "4        0        0                                   1   \n",
      "\n",
      "   submission_of_application_to_inst  Region_North  Region_North-East  \\\n",
      "0                                  1             0                  0   \n",
      "1                                  1             1                  0   \n",
      "2                                  1             0                  0   \n",
      "3                                  0             1                  0   \n",
      "4                                  0             1                  0   \n",
      "\n",
      "   Region_central  Region_south  Security_Type_Indriect  Security_Type_direct  \n",
      "0               0             1                       0                     1  \n",
      "1               0             0                       0                     1  \n",
      "2               0             1                       0                     1  \n",
      "3               0             0                       0                     1  \n",
      "4               0             0                       0                     1  \n",
      "\n",
      "[5 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove the first two rows (ID and year not useful features)\n",
    "df = df.drop(df.columns[[0,1]], axis=1)\n",
    "\n",
    "# Remove outliers using interquartile range method (IQR)\n",
    "# for column in df.columns:\n",
    "#     if pd.api.types.is_numeric_dtype(df[column]):\n",
    "#         Q1 = df[column].quantile(0.25)\n",
    "#         Q3 = df[column].quantile(0.75)\n",
    "#         IQR = Q3 - Q1\n",
    "#         lower_bound = Q1 - 1.5 * IQR\n",
    "#         upper_bound = Q3 + 1.5 * IQR\n",
    "#         df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107, 118936)\n",
      "(1, 118936)\n",
      "(107, 14867)\n",
      "(1, 14867)\n"
     ]
    }
   ],
   "source": [
    "# Convert the data into numpy arrays, ready to be processed by the model\n",
    "X_df = df.drop(columns=[\"Status\"])\n",
    "Y_df = df.iloc[:, 9]\n",
    "X_train_df, X_temp_df, Y_train_df, Y_temp_df = train_test_split(X_df, Y_df, test_size=0.2, random_state=42)\n",
    "X_cv_df, X_test_df, Y_cv_df, Y_test_df = train_test_split(X_temp_df, Y_temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "columns_to_standardize = ['loan_amount', 'rate_of_interest', \"Interest_rate_spread\", \"Upfront_charges\", \"property_value\", \"income\", \"Credit_Score\", \"LTV\", \"dtir1\"]\n",
    "scaler.fit(X_train_df[columns_to_standardize])\n",
    "X_train_df[columns_to_standardize] = scaler.transform(X_train_df[columns_to_standardize])\n",
    "X_cv_df[columns_to_standardize] = scaler.transform(X_cv_df[columns_to_standardize])\n",
    "X_test_df[columns_to_standardize] = scaler.transform(X_test_df[columns_to_standardize])\n",
    "\n",
    "X_train = X_train_df.values.T\n",
    "X_cv = X_cv_df.values.T\n",
    "X_test = X_test_df.values.T\n",
    "\n",
    "Y_train = Y_train_df.values.reshape(1, -1)\n",
    "Y_cv = Y_cv_df.values.reshape(1, -1)\n",
    "Y_test = Y_test_df.values.reshape(1, -1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_cv.shape)\n",
    "print(Y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation methods of the feed-forward neural network (FFN) \n",
    "\n",
    "def forward_prop_layer(W, b, Aprev):\n",
    "    '''\n",
    "    Inputs:  W - weight matrix for current layer (k x n)\n",
    "             A - activation matrix from previous layer (n x m)\n",
    "             b - bias vector for current player (n x 1)\n",
    "\n",
    "    Outputs: Z - pre-activation output matrix resulting from this layer (k, m)\n",
    "    '''\n",
    "\n",
    "    Z_prebatch = np.matmul(W, Aprev) + b\n",
    "    return Z_prebatch\n",
    "\n",
    "def forward_prop_batch_norm_layer(gamma, beta, Z_prebatch):\n",
    "    '''\n",
    "    Inputs:  Gamma - Gamma vector for current batchnorm layer (k x 1)\n",
    "             Beta  - Beta vector for current batchnorm layer (k x 1)\n",
    "             Z     - pre-activation output matrix from regular forward prop layer (k x m)\n",
    "\n",
    "    Outputs: A     - Activation matrix resulting from this layer\n",
    "    '''\n",
    "\n",
    "    u = np.mean(Z_prebatch, axis=1).reshape(-1, 1) # .reshape(-1, 1) makes sure u and var have 2 dimensions (k,1) instead of (k,))\n",
    "    var = np.var(Z_prebatch, axis=1).reshape(-1, 1)\n",
    "\n",
    "    Z_norm = (Z_prebatch - u) / np.sqrt(var + (10 ** (-7)))\n",
    "    Z_updated = np.multiply(gamma, Z_norm) + beta\n",
    "    new_A = relu(Z_updated)\n",
    "\n",
    "    return (new_A, Z_updated, Z_norm, u, var)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters, num_layers):\n",
    "    '''\n",
    "    Inputs:  X - input batch matrix (n x m)\n",
    "             parameters - dictionary containing weight, scale, shift, and bias matrices\n",
    "    Outputs: Y - output vector containing predictions (1 x m)\n",
    "    '''\n",
    "\n",
    "    A = X\n",
    "    cache = {\"Z0\" : np.array([[1]])}\n",
    "\n",
    "    for l in range(1, num_layers + 1):\n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "        gamma = parameters[f\"gamma{l}\"]\n",
    "        beta = parameters[f\"beta{l}\"]\n",
    "\n",
    "        Z = forward_prop_layer(W, b, A)\n",
    "        (A_updated, Z_updated, Z_norm, u, var) = forward_prop_batch_norm_layer(gamma, beta, Z)\n",
    "        \n",
    "        if l < 5: \n",
    "            Z = Z_updated\n",
    "            A = A_updated\n",
    "        else:\n",
    "            A = sigmoid(Z)\n",
    "\n",
    "\n",
    "        cache[f\"A{l}\"] = A\n",
    "        cache[f\"Z{l}\"] = Z\n",
    "        cache[f\"Z_norm{l}\"] = Z_norm\n",
    "        cache[f\"mean{l}\"] = u\n",
    "        cache[f\"var{l}\"] = var\n",
    "\n",
    "\n",
    "    cache[\"A0\"] = X\n",
    "    Y_pred = cache[\"A5\"]\n",
    "\n",
    "    return (Y_pred, cache)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    '''\n",
    "    Inputs: layer_dims - list containing number of neurons for each layer\n",
    "    Outputs: parameters - dictionary containing all the weight, bias, scale, and shift matrices\n",
    "    '''\n",
    "    parameters = {}\n",
    "    adam_cache = {}\n",
    "\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        # Use He initialization for relu (hidden layers) to help alleviate vanishing/exploding gradients \n",
    "        # Use Glorot initialization for sigmoid (output layer) to help alleviate vanishing/exploding gradients\n",
    "        standard_dev = np.sqrt(2.0 / layer_dims[l-1]) if l < len(layer_dims) - 1 else np.sqrt(6.0 / (layer_dims[l-1] + layer_dims[l]))\n",
    "\n",
    "        # Initialize the trainable parameters\n",
    "        parameters[f\"W{l}\"] = np.random.normal(loc=0, scale=standard_dev, size=(layer_dims[l], layer_dims[l-1]))\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "        parameters[f\"gamma{l}\"] = np.ones((layer_dims[l], 1))\n",
    "        parameters[f\"beta{l}\"] = np.ones((layer_dims[l], 1))\n",
    "\n",
    "        # Initialize the adam optimizer parameters as all zeros\n",
    "        adam_cache[f\"VdW{l}\"] = np.zeros_like(parameters[f\"W{l}\"])\n",
    "        adam_cache[f\"SdW{l}\"] = np.zeros_like(parameters[f\"W{l}\"])\n",
    "        adam_cache[f\"Vdb{l}\"] = np.zeros_like(parameters[f\"b{l}\"])\n",
    "        adam_cache[f\"Sdb{l}\"] = np.zeros_like(parameters[f\"b{l}\"])\n",
    "        adam_cache[f\"Vdgamma{l}\"] = np.zeros_like(parameters[f\"gamma{l}\"])\n",
    "        adam_cache[f\"Sdgamma{l}\"] = np.zeros_like(parameters[f\"gamma{l}\"])\n",
    "        adam_cache[f\"Vdbeta{l}\"] = np.zeros_like(parameters[f\"beta{l}\"])\n",
    "        adam_cache[f\"Sdbeta{l}\"] = np.zeros_like(parameters[f\"beta{l}\"])\n",
    "\n",
    "\n",
    "    return (parameters, adam_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_layer_including_batchnorm(dY, mean, variance, gamma, Z_norm, Z, Zprev, Aprev, W, regularization_parameter, is_last_layer):\n",
    "    '''\n",
    "    Inputs: matrices calculated from forward prop for this layer\n",
    "            regularization_parameter: regularization parameter lambda from adding regularization to the model\n",
    "    Outputs: dY_prev_layer - output matrix from batchnorm backprop for this layer\n",
    "    '''\n",
    "\n",
    "    m = Z.shape[1]\n",
    "\n",
    "    # Calculate scale and shift gradients\n",
    "    dbeta = np.sum(dY, axis=1, keepdims=True)\n",
    "    dgamma = np.sum(dY * Z_norm, axis=1, keepdims=True)\n",
    "\n",
    "    # calculate gradient of normalized activations\n",
    "    dZ_norm = dY * gamma\n",
    "\n",
    "    # calculate mean and variance gradients\n",
    "    dvar = np.sum(np.multiply(dZ_norm, Z - mean) * -0.5 * np.power(variance + 10 ** (-7), -1.5), axis=1, keepdims=True)\n",
    "    dmean = np.sum(dZ_norm * -1 / (np.sqrt(variance + 10 ** (-7))), axis=1, keepdims=True)\n",
    "\n",
    "    # Calculate the regular backprop gradients\n",
    "    dZ = dZ_norm / (np.sqrt(variance + 10 ** (-7))) + dvar * 2 * (Z - mean) / m + dmean / m\n",
    "\n",
    "    # no batchnorm for the last layer\n",
    "    if is_last_layer:\n",
    "        dZ = dY\n",
    "        \n",
    "    dW = (1/m) * np.matmul(dZ, Aprev.T) + regularization_parameter * W\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dAprev = np.matmul(W.T, dZ)\n",
    "    if Zprev.shape != (1,1): dY_for_previous_layer = np.multiply(dAprev, relu_derivative(Zprev))\n",
    "    else: dY_for_previous_layer = np.array([[0]])\n",
    "\n",
    "    return (dY_for_previous_layer, dW, db, dgamma, dbeta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(A, Y, forward_prop_cache, parameters, num_layers):\n",
    "    '''\n",
    "    Inputs: Y - vector containing output predictions from forward prop\n",
    "            forward_prop_cache: dictionary containing relevant matrices from forward propagation\n",
    "    Outputs: back_prop_cache: dictionary containing relevant matrices from backpropagation, used for calculating new gradient values\n",
    "    '''\n",
    "\n",
    "    backprop_cache = {}\n",
    "\n",
    "    # Calculate gradient for output layer\n",
    "    backprop_cache[f\"dY{num_layers}\"] = A - Y\n",
    "    dY = A - Y\n",
    "    #max_norm = 5.0\n",
    "\n",
    "    # Update gradients for rest of layers\n",
    "    for l in range(num_layers, 0, -1):\n",
    "        mean = forward_prop_cache[f\"mean{l}\"]\n",
    "        variance = forward_prop_cache[f\"var{l}\"]\n",
    "        gamma = parameters[f\"gamma{l}\"]\n",
    "        beta = parameters[f\"beta{l}\"]\n",
    "        Z_norm = forward_prop_cache[f\"Z_norm{l}\"]\n",
    "        Z = forward_prop_cache[f\"Z{l}\"]\n",
    "        Zprev = forward_prop_cache[f\"Z{l-1}\"]\n",
    "        Aprev = forward_prop_cache[f\"A{l-1}\"]\n",
    "        W = parameters[f\"W{l}\"]\n",
    "\n",
    "        is_last_layer = True if l == num_layers else False\n",
    "        (dY, dW, db, dgamma, dbeta) = backprop_layer_including_batchnorm(dY, mean, variance, gamma, Z_norm, Z, Zprev, Aprev, W, 0.0001, is_last_layer)\n",
    "\n",
    "        backprop_cache[f\"dY{l-1}\"] = dY\n",
    "        backprop_cache[f\"dW{l}\"] = dW\n",
    "        backprop_cache[f\"db{l}\"] = db\n",
    "        backprop_cache[f\"dgamma{l}\"] = dgamma\n",
    "        backprop_cache[f\"dbeta{l}\"] = dbeta\n",
    "\n",
    "\n",
    "    return backprop_cache\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_cross_entropy_cost_without_regularization(y, yhat):\n",
    "    '''\n",
    "    Input: y - row vector  containing actual labels of the training examples\n",
    "           yhat - row vector containing predicted probabilities by model\n",
    "    Output: total cost of predictions\n",
    "    '''\n",
    "    # Clip yhat to avoid log(0) errors (log(0) = -inf)\n",
    "    yhat = np.clip(yhat, 1e-15, 1 - 1e-15)\n",
    "    # Compute binary cross-entropy (element-wise)\n",
    "    cost = -np.mean(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate=0.001, num_iterations=60000):\n",
    "    '''\n",
    "    Hidden Layer 1: 220 neurons\n",
    "    Hidden Layer 2: 100 neurons\n",
    "    Hidden Layer 3: 50 neurons\n",
    "    Hidden Layer 4: 30 neurons\n",
    "    Hidden Layer 5: 1 neuron (sigmoid activation determining output for binary classification)\n",
    "\n",
    "    Inputs: X - input matrix containing training examples\n",
    "            Y - row vector containing the correct labels of the training examples\n",
    "            learning_rate: initial learning rate (alpha) of the model\n",
    "            num_iterations: \n",
    "    '''\n",
    "    layer_dims = [X.shape[0], 220, 100, 50, 30, 1]\n",
    "    (parameters, adam_cache) = initialize_parameters(layer_dims)\n",
    "    num_layers = len(layer_dims) - 1\n",
    "\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-7 \n",
    "    \n",
    "    # Split the dataset into mini-batches\n",
    "    shuffled_indices = np.random.permutation(X.shape[1])\n",
    "    X = X[:, shuffled_indices]\n",
    "    Y = Y[:, shuffled_indices]\n",
    "    m = X.shape[1]\n",
    "    batch_size = 512\n",
    "    num_batches = int(np.ceil(m / batch_size))\n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i + 1) * batch_size, m)\n",
    "        curr_batch_X = X[:, start_index:end_index]\n",
    "        curr_batch_Y = Y[:, start_index:end_index]\n",
    "        batches.append((curr_batch_X, curr_batch_Y))\n",
    "\n",
    "    batch_index = 0\n",
    "    curr_epoch = -1\n",
    "\n",
    "    for i in range(num_iterations + 1):\n",
    "        (batch_X, batch_Y) = batches[batch_index]\n",
    "        batch_index = (batch_index + 1) % num_batches\n",
    "        if batch_index == 0: curr_epoch += 1\n",
    "\n",
    "        min_learning_rate = 1e-6\n",
    "        curr_learning_rate = max(min_learning_rate, learning_rate * np.exp(-0.96 * curr_epoch))\n",
    "\n",
    "        # Perform forward propagation\n",
    "        (yhat, forward_prop_cache) = forward_prop(np.array(batch_X), parameters, num_layers)\n",
    "\n",
    "        # perform backpropagation\n",
    "        A_last_layer = forward_prop_cache[f\"A{num_layers}\"]\n",
    "        backprop_cache = backprop(A_last_layer, np.array(batch_Y), forward_prop_cache, parameters, num_layers)\n",
    "    \n",
    "        # Update trainable parameters using Adam optimizer for all the layers\n",
    "        for l in range(1, num_layers + 1):\n",
    "            dW = backprop_cache[f\"dW{l}\"]\n",
    "            db = backprop_cache[f\"db{l}\"]\n",
    "            dgamma = backprop_cache[f\"dgamma{l}\"]\n",
    "            dbeta = backprop_cache[f\"dbeta{l}\"]\n",
    "\n",
    "            VdW = adam_cache[f\"VdW{l}\"]\n",
    "            SdW = adam_cache[f\"SdW{l}\"]\n",
    "            Vdb = adam_cache[f\"Vdb{l}\"]\n",
    "            Sdb = adam_cache[f\"Sdb{l}\"]\n",
    "            Vdgamma = adam_cache[f\"Vdgamma{l}\"]\n",
    "            Sdgamma = adam_cache[f\"Sdgamma{l}\"]\n",
    "            Vdbeta = adam_cache[f\"Vdbeta{l}\"]\n",
    "            Sdbeta = adam_cache[f\"Sdbeta{l}\"]\n",
    "            \n",
    "            adam_cache[f\"VdW{l}\"] = (beta1 * VdW + (1. - beta1) * dW)\n",
    "            adam_cache[f\"Vdb{l}\"] = (beta1 * Vdb + (1. - beta1) * db)\n",
    "            adam_cache[f\"SdW{l}\"] = (beta2 * SdW + (1. - beta2) * (dW ** 2))\n",
    "            adam_cache[f\"Sdb{l}\"] = (beta2 * Sdb + (1. - beta2) * (db ** 2))\n",
    "            adam_cache[f\"Vdgamma{l}\"] = (beta1 * Vdgamma + (1. - beta1) * dgamma)\n",
    "            adam_cache[f\"Vdbeta{l}\"] = (beta1 * Vdbeta + (1. - beta1) * dbeta)\n",
    "            adam_cache[f\"Sdgamma{l}\"] = (beta2 * Sdgamma + (1. - beta2) * (dgamma ** 2))\n",
    "            adam_cache[f\"Sdbeta{l}\"] = (beta2 * Sdbeta + (1. - beta2) * (dbeta ** 2))\n",
    "\n",
    "            parameters[f\"W{l}\"] -= learning_rate * adam_cache[f\"VdW{l}\"] / (np.sqrt(adam_cache[f\"SdW{l}\"]) + epsilon)\n",
    "            parameters[f\"b{l}\"] -= learning_rate * adam_cache[f\"Vdb{l}\"] / (np.sqrt(adam_cache[f\"Sdb{l}\"]) + epsilon)\n",
    "            parameters[f\"gamma{l}\"] -= learning_rate * adam_cache[f\"Vdgamma{l}\"] / (np.sqrt(adam_cache[f\"Sdgamma{l}\"]) + epsilon)\n",
    "            parameters[f\"beta{l}\"] -= learning_rate * adam_cache[f\"Vdbeta{l}\"] / (np.sqrt(adam_cache[f\"Sdbeta{l}\"]) + epsilon)\n",
    "\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Current loss for iteration {i}: {compute_binary_cross_entropy_cost_without_regularization(batch_Y, yhat)}\")\n",
    "        \n",
    "\n",
    "            \n",
    "    return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inferences(X_input, Y_output, parameters):\n",
    "    (Y_test_predictions, _) = forward_prop(X_input, parameters, 5)\n",
    "    Y_test_predictions = (Y_test_predictions >= 0.5).astype(int)\n",
    "    accuracy = np.sum(Y_test_predictions == Y_output) / Y_output.shape[1]\n",
    "    return (accuracy, Y_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss for iteration 0: 1.1836756067005112\n",
      "Current loss for iteration 1000: 0.3960378725094277\n",
      "Current loss for iteration 2000: 0.22784225409457046\n",
      "Current loss for iteration 3000: 0.17619029061668623\n",
      "Current loss for iteration 4000: 0.18216429235222448\n",
      "Current loss for iteration 5000: 0.10414489897263797\n",
      "Current loss for iteration 6000: 0.19193754744101318\n",
      "Current loss for iteration 7000: 0.15970130557893447\n",
      "Current loss for iteration 8000: 0.08449635169678574\n",
      "Current loss for iteration 9000: 0.05201329071935923\n",
      "Current loss for iteration 10000: 0.05668125170625836\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, num_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f1 score\n",
    "def confusion_matrix_calculations(Y_pred, Y):\n",
    "    true_positives = np.sum((Y_pred == 1) & (Y == 1))\n",
    "    false_positives = np.sum((Y_pred == 1) & (Y == 0))\n",
    "    false_negatives = np.sum((Y_pred == 0) & (Y == 1))\n",
    "\n",
    "    if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    else:\n",
    "        precision = 0\n",
    "\n",
    "    if true_positives + false_negatives > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * precision * recall / (precision + recall) \n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "    return f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation accuracy: 0.9809645523642968\n",
      "Cross Validation f1_score: 0.9602025031641119\n"
     ]
    }
   ],
   "source": [
    "cv_accuracy, y_cv_predictions = make_inferences(X_cv, Y_cv, parameters)\n",
    "print(\"Cross Validation accuracy: \" + str(cv_accuracy))\n",
    "f1_score = confusion_matrix_calculations(y_cv_predictions, Y_cv)\n",
    "print(\"Cross Validation f1_score: \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Validation accuracy: 0.9827133920764108\n",
      "Test f1_score: 0.963374661536269\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, y_test_predictions = make_inferences(X_test, Y_test, parameters)\n",
    "print(\"Test Validation accuracy: \" + str(test_accuracy))\n",
    "f1_score = confusion_matrix_calculations(y_test_predictions, Y_test)\n",
    "print(\"Test f1_score: \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
